{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cd7bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import logm, expm, eigh\n",
    "import os\n",
    "import gseapy as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b72b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spd(A, tol=1e-8):\n",
    "    # Check symmetry\n",
    "    if not np.allclose(A, A.T, atol=tol):\n",
    "        return False\n",
    "    # Check eigenvalues > 0\n",
    "    eigvals = np.linalg.eigvalsh(A)\n",
    "    return np.all(eigvals > tol)\n",
    "\n",
    "def project_to_spd(A, tol=1e-8):\n",
    "    # Make symmetric\n",
    "    A = (A + A.T) / 2\n",
    "    eigvals, eigvecs = eigh(A)\n",
    "    eigvals_clipped = np.clip(eigvals, tol, None)  # set eigenvalues < tol to tol\n",
    "    return eigvecs @ np.diag(eigvals_clipped) @ eigvecs.T\n",
    "\n",
    "def make_psd(K, min_eig=1e-6):\n",
    "    K = (K + K.T) / 2\n",
    "    eigvals = np.linalg.eigvalsh(K)\n",
    "    if np.min(eigvals) < min_eig:\n",
    "        K += np.eye(K.shape[0]) * (min_eig - np.min(eigvals))\n",
    "    return K\n",
    "\n",
    "def weighted_log_euclidean_mean(kernels, weights):\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    log_sum = np.zeros_like(kernels[0])\n",
    "    for w, K in zip(weights, kernels):\n",
    "        K = make_psd(K)\n",
    "        log_K = logm(K)\n",
    "        # Handle potential complex results more carefully\n",
    "        if np.iscomplexobj(log_K) and np.allclose(log_K.imag, 0, atol=1e-10):\n",
    "            log_K = log_K.real\n",
    "        log_sum += w * log_K\n",
    "    \n",
    "    return expm(log_sum)\n",
    "\n",
    "def read_data(disease, dga, features):\n",
    "    columns_to_keep = [col for col in features.columns if col.startswith('feature') or col.startswith('string')]\n",
    "    df = features[columns_to_keep]\n",
    "\n",
    "    pos_genes_list = dga[dga['disease_id']==disease]['string_id']\n",
    "    df['label'] = df['string_id'].isin(pos_genes_list).astype(int)\n",
    "\n",
    "    # X = df.loc[:, df.columns.str.startswith(\"feature_\")].to_numpy()\n",
    "    y = df['label'].to_numpy()\n",
    "    df.set_index('string_id', inplace=True)\n",
    "    df.drop(columns='label', inplace=True)\n",
    "    return df, y\n",
    "\n",
    "def read_data_timecut(disease, dga, features,time):\n",
    "    pos_genes_list = dga[dga['disease_id']==disease]['string_id']\n",
    "    columns_to_keep = [col for col in features.columns if 'feature' in col or col.startswith('string')]\n",
    "    df = features[columns_to_keep]\n",
    "    df['label'] = df['string_id'].isin(pos_genes_list).astype(int)\n",
    "    df['test'] = df['string_id'].isin(dga[(dga['disease_id'] == disease) & (dga['first_pub_year'] > time)]['string_id']).astype(int)\n",
    "\n",
    "    # X = df.loc[:, df.columns.str.startswith(\"feature_\")].to_numpy()\n",
    "    y = df['label'].to_numpy()\n",
    "    df.set_index('string_id', inplace=True)\n",
    "    df.drop(columns='label', inplace=True)\n",
    "    return df, y\n",
    "\n",
    "\n",
    "def get_feature(root, feature_name):\n",
    "    if feature_name == 'ppi_align':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/ppi_full_emb_aligned.csv'))\n",
    "    elif feature_name == 'ppi':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/ppi_full_emb.csv'))\n",
    "    elif feature_name == 'biograd':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/biograd/biograd_full_emb.csv'))\n",
    "    elif feature_name == 'prose':\n",
    "        feature_df = pd.read_csv(os.path.join(root, 'data/prose/data/prose_emb_full.csv'))\n",
    "    # elif feature_name == 'ppi_400':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/ppi_full_400_emb.csv'))\n",
    "    elif feature_name == 'ppi_2016':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/ppi_full_2016_emb.csv'))\n",
    "    elif feature_name == 'ppi_2019':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/ppi_full_2019_emb.csv'))\n",
    "    # elif feature_name == 'ppi_2013':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/ppi_full_2013_emb.csv'))\n",
    "    elif feature_name == 'uniport':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/seq_emb/human_uniport_seqemb.csv'))\n",
    "    # elif feature_name == 't5_align':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/T5/T5_align.csv'))\n",
    "    elif feature_name == 'gene2vec':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/expression_emb/exp_emb.csv'))\n",
    "        # feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/GENE2VEC/GENE2VEC_align.csv'))\n",
    "    \n",
    "    elif feature_name == 'scgpt':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/scgpt/scgpt_full.csv'))\n",
    "        # feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/expression_emb/SCGPT-HUMAN/scgpt.csv'))\n",
    "    elif feature_name == 'bioconcept':\n",
    "        # feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/text_minning_2015/text_features.csv'))\n",
    "        # feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/txt/BIOCONCEPTVEC-FASTTEXT/text.csv'))\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/bioconcept/bioconcept_full.csv'))\n",
    "    # elif feature_name == 'go_svd':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/GO/GO_2023_features.csv'))\n",
    "    # elif feature_name == 'go_all':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/GO/GO_2023_all_features_aligned.csv'))\n",
    "    elif feature_name == 'esm2':\n",
    "        feature_df = pd.read_csv(os.path.join(root,'data/esmfold/esm2.csv'))\n",
    "        # feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/ESM2/ESM2_align.csv'))\n",
    "    # elif feature_name == 'MASHUP':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/MASHUP/MASHUP_align.csv'))\n",
    "    # elif feature_name == 'GENEPT_MODEL3':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/GENEPT_MODEL3/GENEPT_MODEL3_align.csv'))    \n",
    "    # elif feature_name == 'GF_12L95M':\n",
    "    #     feature_df = pd.read_csv(os.path.join(root,'data/pre_processed_features/GF_12L95M/GF_12L95M_align.csv')) \n",
    "\n",
    "    return feature_df\n",
    "\n",
    "import pickle\n",
    "with open('/itf-fi-ml/shared/users/ziyuzh/svm/data/stringdb/2023/name_convert.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "stringId2name,name2stringId,aliases2stringId = loaded_data\n",
    "del name2stringId,aliases2stringId\n",
    "\n",
    "def enriched_set(input_stringids,time):\n",
    "    gene_names = [stringId2name.get(sid) for sid in input_stringids if stringId2name.get(sid) is not None]\n",
    "    if time == 2019:\n",
    "        enrich_db = ['GO_Biological_Process_2021','GO_Cellular_Component_2021','GO_Molecular_Function_2021','KEGG_2019_Human']\n",
    "    elif time == 2017:\n",
    "        enrich_db = ['GO_Biological_Process_2021','GO_Cellular_Component_2021','GO_Molecular_Function_2021','KEGG_2016']\n",
    "    enr = gp.enrichr(gene_list=gene_names,\n",
    "                    gene_sets=enrich_db,\n",
    "                    organism='human', \n",
    "                    outdir=None, \n",
    "                    )\n",
    "    enr_df = enr.results\n",
    "    result_terms = enr_df.loc[enr_df['Adjusted P-value'] < 0.01, ['Gene_set', 'Term']]\n",
    "    return set(map(tuple, result_terms.values))\n",
    "\n",
    "def calculate_jac_sim(enrich_1, enrich_2):\n",
    "    intersection = enrich_1 & enrich_2\n",
    "    union = enrich_1 | enrich_2\n",
    "    jaccard_similarity = len(intersection) / len(union)\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094979bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ppi_2019', 'bioconcept'] 42 12343\n",
      "ICD10_C16 69\n"
     ]
    }
   ],
   "source": [
    "time_spilt = True\n",
    "time = 2019\n",
    "feature_list = ['ppi_2019','bioconcept']\n",
    "root = '/itf-fi-ml/shared/users/ziyuzh/svm'\n",
    "\n",
    "merged_df = None\n",
    "for feature in feature_list:\n",
    "    feature_df = get_feature(root, feature)\n",
    "    # Rename columns starting with 'feature'\n",
    "    feature_df.rename(columns={\n",
    "        col: f\"{feature}_{col}\" if col.startswith('feature') else col\n",
    "        for col in feature_df.columns\n",
    "    }, inplace=True)\n",
    "    # Merge iteratively to avoid keeping all DataFrames\n",
    "    if merged_df is None:\n",
    "        merged_df = feature_df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, feature_df, on='string_id', how='inner')\n",
    "    del feature_df  # Free memory\n",
    "\n",
    "all_df = pd.read_csv('/itf-fi-ml/shared/users/ziyuzh/svm/data/disgent_2020/timecut/disgent_with_time.csv')\n",
    "all_df = all_df[all_df['string_id'].isin(merged_df['string_id'])]\n",
    "\n",
    "if time_spilt:\n",
    "    selected_diseases = []\n",
    "    for disease_id in all_df['disease_id'].unique():\n",
    "        sub_df = all_df[all_df['disease_id']==disease_id]\n",
    "        if len(sub_df) < 15:\n",
    "            continue\n",
    "        else:\n",
    "            # print(type(time),type(sub_df['first_pub_year'].max()))\n",
    "            if sub_df['first_pub_year'].max() > time and sub_df['first_pub_year'].min() <= time and len(sub_df[sub_df['first_pub_year']<time]) >=5:\n",
    "                selected_diseases.append(disease_id)\n",
    "else:\n",
    "    selected_diseases = (\n",
    "        all_df.groupby('disease_id')\n",
    "        .filter(lambda x: (len(x) > 15))\n",
    "        ['disease_id']\n",
    "        .unique()\n",
    "        .tolist())\n",
    "print(feature_list, len(selected_diseases),len(merged_df))\n",
    "all_results = []\n",
    "\n",
    "for disease in selected_diseases[:1]:\n",
    "    print(disease,len(all_df[all_df['disease_id']==disease]))\n",
    "    if time_spilt:\n",
    "        df, y = read_data_timecut(disease, all_df, merged_df,time)\n",
    "    else:\n",
    "        df, y = read_data(disease, all_df, merged_df,time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9767290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = df[df['test']==1].index\n",
    "train_idx = df[y==1].index.difference(test_idx)\n",
    "df.drop(columns='test', inplace=True)\n",
    "\n",
    "train_pos_df = df.loc[train_idx]\n",
    "test_pos_df = df.loc[test_idx]\n",
    "neg_num = 5*len(train_pos_df)\n",
    "\n",
    "if 'linear_fused' in feature_list:\n",
    "    feature_list.remove('linear_fused')\n",
    "if 'geo_fused' in feature_list:\n",
    "    feature_list.remove('geo_fused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d53b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5100bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'linear_fused' in feature_list:\n",
    "    feature_list.remove('linear_fused')\n",
    "if 'geo_fused' in feature_list:\n",
    "    feature_list.remove('geo_fused')\n",
    "\n",
    "\n",
    "# Work with DataFrames to maintain indices\n",
    "neg_df = df[y == 0]\n",
    "\n",
    "# Randomly select 'neg_num' samples from negative class\n",
    "train_neg_df = neg_df.sample(n=neg_num, random_state=42)\n",
    "\n",
    "# Get the all negative samples\n",
    "test_neg_df = neg_df\n",
    "\n",
    "# Combine positive and negative samples for training\n",
    "train_df = pd.concat([train_pos_df, train_neg_df])\n",
    "test_df = pd.concat([test_pos_df, test_neg_df])\n",
    "\n",
    "X_train_mats = []\n",
    "X_test_mats = []\n",
    "for feature_name in feature_list:\n",
    "    select_columns = [col for col in df.columns if col.startswith(feature_name)]\n",
    "    X_train_mats.append(train_df[select_columns].values)\n",
    "    X_test_mats.append(test_df[select_columns].values)\n",
    "\n",
    "feature_weights = [1 / len(feature_list)] * len(feature_list)\n",
    "\n",
    "y_train = np.array([1] * len(train_pos_df) + [0] * len(train_neg_df))\n",
    "y_test = np.array([1] * len(test_pos_df) + [0] * len(test_neg_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2e45a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_all = []\n",
    "kernels_train = []\n",
    "kernels_test = []\n",
    "\n",
    "# For each feature set\n",
    "for X_tr, X_te in zip(X_train_mats, X_test_mats):\n",
    "    X_all = np.concatenate([X_tr, X_te], axis=0)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=2).fit(X_all)\n",
    "    distances, _ = nbrs.kneighbors(X_all)\n",
    "    avg_nn_dist = np.mean(distances[:, 1])  # skip self-distance\n",
    "    gamma = 1 / (2 * avg_nn_dist ** 2)\n",
    "    K_full = rbf_kernel(X_all, X_all, gamma=gamma)\n",
    "    kernels_all.append(K_full)\n",
    "\n",
    "    n_train = len(X_tr)\n",
    "    kernels_train.append(K_full[:n_train, :n_train])\n",
    "    kernels_test.append(K_full[n_train:, :n_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4af893",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = test_df.index.values\n",
    "enrich_train_genes = train_pos_df.index.values\n",
    "enrich_train_set = enriched_set(enrich_train_genes,time)\n",
    "\n",
    "pathway_overlap = []\n",
    "for feature_index, feature_name in enumerate(feature_list):\n",
    "    best_svm = svm.SVC(kernel='precomputed')\n",
    "    best_svm.fit(kernels_train[feature_index], y_train)\n",
    "    y_scores = best_svm.decision_function(kernels_test[feature_index])\n",
    "\n",
    "    ############################## Add results to df here \n",
    "    enrich_test_genes = test_indices[np.argsort(y_scores)[::-1]][:200]\n",
    "    enrich_feature_test = enriched_set(enrich_test_genes,time)\n",
    "    jac_sm = calculate_jac_sim(enrich_train_set,enrich_feature_test)\n",
    "    pathway_overlap.append(jac_sm)\n",
    "\n",
    "total = sum(pathway_overlap)\n",
    "feature_weights = [v / total for v in pathway_overlap]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298cc59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_linear_all = sum(w * K_train_i for w, K_train_i in zip(feature_weights, kernels_all))\n",
    "kernels_train.append(K_linear_all[:n_train, :n_train])\n",
    "kernels_test.append(K_linear_all[n_train:, :n_train])\n",
    "feature_list.append('linear_fused')\n",
    "\n",
    "# K_geo_all = weighted_log_euclidean_mean(kernels_all, feature_weights)\n",
    "# kernels_train.append(K_geo_all[:n_train, :n_train])\n",
    "# kernels_test.append(K_geo_all[n_train:, :n_train])\n",
    "# feature_list.append('geo_fused')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d375fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_index, feature_name in enumerate(feature_list[-1:]):\n",
    "    best_svm = svm.SVC(kernel='precomputed')\n",
    "    best_svm.fit(kernels_train[feature_index], y_train)\n",
    "    y_scores = best_svm.decision_function(kernels_test[feature_index])\n",
    "    ############################ add resluts to df here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20828633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09916589434661724, 0.26582278481012656]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12e9815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ppi_2019', 'bioconcept', 'linear_fused'],\n",
       " [0.27169580869118026, 0.7283041913088196])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list, feature_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
